
<html>
<head>
<title>jhc</title>
<link rel=stylesheet type="text/css" href="/style/blue-style.css">
</head>
<body>
    <h1 class=maintitle style="visibility: hidden">Jhc</h1>
<!--#include virtual="/try/navbar.html" -->
<div class=foo>

<div id="jhc"
><h1
  >Jhc</h1
  ><p
  >Jhc is a compiler for Haskell that aims to produce very efficient code as well as explore novel compilation techniques in an attempt to make them practical.</p
  ><p
  >One thing jhc does not aim to be is a toy or proof-of-concept compiler. A lot of the techniques have already had proof-of-concept implementations and jhc aims to determine how to bring them to a full-scale Haskell compiler. (or die trying)</p
  ><p
  >Although jhc is not ready for general use, I think some of its ideas or code might be useful to other people so I am deciding to release it in this state.</p
  ><div id="jhc-bullet-points"
  ><h2
    >Jhc Bullet Points</h2
    ><ul
    ><li
      ><p
	>Full support for Haskell 98, The FFI and some extensions. (modulo some bugs being worked on and some libraries that need to be filled out)</p
	></li
      ><li
      ><p
	>Produces 100% portable ISO C. The same C file can compile on machines of different byte order or bit-width without trouble.</p
	></li
      ><li
      ><p
	>No pre-written runtime. other than 20 lines of boilerplate all code is generated from the Grin intermediate code and subject to all code simplifying and dead code elimination transformations. As a result, jhc currently produces the smallest binaries of any Haskell compiler. (main = putStrLn &quot;Hello, World!&quot; compiles to 6,568 bytes vs 177,120 bytes for GHC 6.4)</p
	></li
      ><li
      ><p
	>No garbage collector. A variant of Region Inference is in the works.</p
	></li
      ><li
      ><p
	>Compilation by transformation with 2 general intermediate languages.</p
	></li
      ><li
      ><p
	>First Intermediate language based on Henk, Pure Type Systems and the Lambda cube. This is similar enough to GHCs core that many optimizations may be implemented in the same way.</p
	></li
      ><li
      ><p
	>Second Intermediate language is based on Boquist's graph reduction language. This allows all unknown jumps to be compiled out leaving standard case statements and function calls as the only form of flow control. Combined with jhc's use of region inference, this means jhc can compile to most any standard imperative architecture/language/virtual machine directly without special support for a stack or tail-calls.</p
	></li
      ><li
      ><p
	>Novel type class implementation not based on dictionary passing with many attractive properties. This implementation is possible due to the whole-program analysis phase and the use of the lambda-cube rather than System F as the base for the functional intermediate language.</p
	></li
      ><li
      ><p
	>Intermediate language and back-end suitable for directly compiling any language that can be embedded in the full lambda cube, making things like a compiler for cayenne much more direct. There is no type erasure phase, types are erased for the simple reason that values do not depend on them via the standard dead-code elimination pass.</p
	></li
      ><li
      ><p
	>A very modern design, it using rank-n polymorphism, monad transformers, generic programing, and existential types to make the code very concise and clear and improve code reuseability. (since jhc was written in pieces over 5 years, some at times when I just started using Haskell, the code quality actually varies a lot across the whole project)</p
	></li
      ><li
      ><p
	>All indirect jumps are transformed away, jhc's final code is very similar to hand-written imperative code, using only branches and static function calls. A simple basic-blocks analysis is enough to transform tail-calls into loops.</p
	></li
      ><li
      ><p
	>Full transparent support for mutually recursive modules.</p
	></li
      ></ul
    ></div
  ></div
><div id="more-in-depth"
><h1
  >More in depth</h1
  ><div id="type-classes"
  ><h2
    >Type Classes</h2
    ><p
    >One of the most novel things about jhc is its implementation of type classes which differs significantly from the standard dictionary passing approach used by other Haskell compilers.</p
    ><p
    >Jhc's unique approach is made possible by 2 other design choices, the use of a pure type system with no distinction between types and values and its use of whole-program analysis. The basic idea is that instead of passing a dictionary, a case statement directly scrutinizes the type parameter of a function and calls the appropriate overloaded routine directly.</p
    ><p
    >This has a number of interesting properties</p
    ><ul
    ><li
      ><p
	>The number of extra hidden parameters is the number of free type variables in a functions signature rather than the number of class constraints. So (Ord a, Show a, Num a) =&gt; a -&gt; a will only pass a single extra parameter for the type of a rather than 3 dictionaries.</p
	></li
      ><li
      ><p
	>2 indirections, first one to look up the dictionary, then one to call the function pointed to in the dictionary are replaced by a single case of an algebraic data type and calls to statically known functions. This is exactly the transformation that the GRIN points-to analysis does, but much sooner and with much better optimization potential. Calls to statically known functions are MUCH more efficient.</p
	></li
      ><li
      ><p
	>Standard case coalescing optimizations have a dramatically enhanced effect when dealing with overloaded functions. imagine the code snipped <span class="math"
	  >(<em
	    >x</em
	    >*(<em
	    >y</em
	    >+<em
	    >z</em
	    >)/<em
	    >z</em
	    >)::<em
	    >a</em
	    ></span
	  > . Each of the calls to the polymorphic functions *, +, and / will expand to a case statement on 'a', since all case statements are trivially examining the same value, they are coalesced into a single one. With dictionary passing, we would have to look up the appropriate entry in the Num hidden parameter, the Floating hidden parameter, then look up each of *, +, and / individually. Under jhc's scheme all of that is statically transformed into a single case on a normal algebraic type. This optimization is a HUGE win. Jhc's ability to do this comes from the fact that it is statically evident from that case statement that the type fully determines every polymorphic function on that type, a property that is lost in the dictionary passing approach since as far as the compiler is concerned, arbitrary functions may be being passed in the dictionaries, it does not know that they come in specific correlated sets.</p
	></li
      ><li
      ><p
	>Functional Dependencies actually lead to run-time savings. each functional dependency transforms into a case statement which may be omitted.</p
	></li
      ><li
      ><p
	>Although a whole-world analysis is needed to generate full versions of type class methods, this is actually rarely needed in practice, as it is often the case the compiler is able to statically prove only a certain subset of types are needed at any given point and is able to generate specialized versions on the spot. This is implemented in a manner very similar to GHC's rules.</p
	></li
      ><li
      ><p
	>Advanced compile-time and run-time specializations are possible via pragmas. (see below)</p
	></li
      ></ul
    ></div
  ><div id="e"
  ><h2
    >E</h2
    ><p
    >E is a pure type system based on Henk and the lambda-cube. An important property of E is that there is no distinction between types and values, this is important for jhc's implementation of type classes.</p
    ></div
  ><div id="grin"
  ><h2
    >Grin</h2
    ><p
    >Grin is basically a first-order monadic functional language. It is very similar to the Graph Reduction Intermediate Language as defined by Boquist but has a few notable changes.</p
    ><ul
    ><li
      ><p
	>It is typed.</p
	></li
      ><li
      ><p
	>It has multiple return values as a primitive (unboxed tuples)</p
	></li
      ><li
      ><p
	>My target is higher level C or C-- rather than RISC code, so some transformations are less important as the C compiler can be assumed to take care of them.</p
	></li
      ><li
      ><p
	>The terminology and syntax borrows from Haskell's current implementation of monads and 'do' notation.</p
	></li
      ></ul
    ><p
    >Most of the transformations mentioned in Boquist's thesis have been implemented, however certain intermediate states in Boquist's scheme are actually invalid in the strongly typed Grin of jhc so need to be combined or modified.</p
    ><p
    >A whole lot can be learned from the Grin data type and Grin is fully defined by the following</p
    ><pre
    ><code
      >infixr 1  :-&gt;, :&gt;&gt;=

data Lam = Val :-&gt; Exp
   deriving(Eq,Ord,Show)

data Exp =
    Exp :&gt;&gt;= !Lam
   | App Atom [Val]  -- ^ this handles applications of functions and built-ins
   | Prim Primitive [Val]
   | Case Val [Lam]
   | Return { expValue :: Val }
   | Store { expValue :: Val }
   | Fetch { expAddress :: Val }
   | Update { expAddress :: Val, expValue :: Val }
   | Error String Ty -- ^ abort with an error message, non recoverably.
   | Cast Val Ty     -- ^ reinterpret Val as a different type, also used to box\/unbox lifted types
   deriving(Eq,Show,Ord)

data Val =
   NodeC !Tag [Val]
   | NodeV !Var [Val]
   | Tag !Tag
   | Const Val         -- ^ pointer to constant data, only Lit, Tag, and NodeC may be children
   | Lit !Number Ty
   | Var !Var Ty
   | Tup [Val]
   | ValPrim APrim
   deriving(Eq,Show,Ord)

data Ty =
   TyTag           -- ^ a lone tag
   | TyPtr Ty      -- ^ pointer to a heap location which contains its argument
   | TyNode        -- ^ a whole tagged node
   | Ty Atom       -- ^ a basic type
   | TyTup [Ty]    -- ^ unboxed list of values
   | TyUnknown     -- ^ an unknown possibly undefined type, All of these must be eliminated by code generation
   deriving(Eq,Ord)
</code
      ></pre
    ></div
  ><div id="extensions"
  ><h2
    >Extensions</h2
    ><p
    >Jhc implements several extensions to Haskell 98</p
    ></div
  ><div id="standard-extensions"
  ><h2
    >Standard Extensions</h2
    ><ul
    ><li
      ><p
	>The FFI is almost fully supported except for calling Haskell code from C.</p
	></li
      ><li
      ><p
	>Hierarchical module names are supported as described in the addendum. The search algorithm is somewhat different than GHC though, Control.Monad.Identity will be searched for as Control/Monad/Identity.hs, Control/Monad.Identity.hs and Control.Monad.Identity.hs, giving you a bit more freedom in laying out your directory structure.</p
	></li
      ><li
      ><p
	>Empty data declarations with no constructors are supported</p
	></li
      ><li
      ><p
	>Liberalized type synonyms are supported (type synonyms may appear anywhere a type may appear)</p
	></li
      ><li
      ><p
	>INLINE, SPECIALIZE pragmas</p
	></li
      ><li
      ><p
	>unsafePerformIO, unsafeInterleaveIO are provided.</p
	></li
      ></ul
    ></div
  ><div id="new-extensions"
  ><h2
    >New Extensions</h2
    ><ul
    ><li
      ><p
	>foreign primitive, all primitives are brought into scope with a foreign primitive declaration, these can also be used to gain access to C constants, obviating much of the need for a preprocessor such as hsc2hs and allowing portable C code to be generated by jhc.</p
	></li
      ><li
      ><p
	>SRCLOC_ANNOTATE pragma. This is a generalization of GHCs assert magic. A function which is given an SRCLOC_ANNOTATE pragma will be replaced with an alternate version that takes the functions use site as an argument. This allows error messages to be in terms of where said function was used. The alternate function is named [function]_srcloc<em
	  >ann</em
	  >_ and must be in the same module as the original function. Jhc does no checking to ensure both functions have the same effect, it is up to the user to ensure that. An example is</p
	><pre
	><code
	  >head :: [a] -&gt; a
head (x:xs) = x
head [] = error &quot;head - empty list&quot;

{-# SRCLOC_ANNOTATE head #-}

head_srcloc_ann__ :: String -&gt; [a] -&gt; a
head_srcloc_ann__ pos (x:xs) = x
head_srcloc_ann__ pos [] = error $ pos ++ &quot;: head - empty list&quot;

-- Now, a call to head on an empty list will produce an error message like
-- &quot;Main.hs:23: head - empty list&quot;
</code
	  ></pre
	></li
      ><li
      ><p
	>SUPERSPECIALIZE pragma. This pragma has the same affect as the SPECIALIZE pragma, but in addition to doing compile-time specialization, SUPERSPECIALIZE performs run-time specialization. A superspecialized function will perform a single check against the type it is called with and depending on the single test, call a specialized version of the function. This can be a huge win when working with overloaded numeric types, imagine a matrix-multiply routine, if the type cannot be determined at compile-time then we would normally be forced to fall back to generic version which may have hundreds of additions and multiplications, each of which must test what type its argument are. If we SUPERSPECIALIZE the multiply routine instead, a single run-time test will be performed and the much much more efficient specialized routine will be used, even if it could not be proven at compile time.</p
	></li
      ><li
      ><p
	>MULTISPECIALIZE pragma. This is equivalent to calling SPECIALIZE against every possible type. It's main cost is compile time and memory usage. It should be used only sparingly as it can lead to quadratic rule explosion in the total number of types in the transitive closure of all imported modules in the worst case.</p
	></li
      ><li
      ><p
	>MULTISUPERSPECIALIZE pragma. This is equivalent to calling SUPERSPECIALIZE against every possible type. If not careful, this can result in massive code bloat but might be a big win in certain cases.</p
	></li
      ></ul
    ></div
  ><div id="the-story-of-jhc"
  ><h2
    >The story of jhc</h2
    ><p
    >When I first started to learn Haskell in 1999, I decided I needed a project. Haskell was my first (modern) functional language and I was seduced by its robust strong type system and efficiency gains. After writing a toy ray-tracer (my usual first project in a new language) it was clear I needed to try something somewhat more challenging and jhc was born. My reasoning was simple, by writing a Haskell compiler in Haskell I will double my language learning speed since I will not only have to learn how to program in it by forcing myself to complete a non-trivial project, but also its subtle semantics and dark corners since I actually needed to implement it correctly. Writing a compiler is also doubly efficient to begin with, since if you self-compile improvements not only give you a better optimizer, but also speed up your self-compiled compiler. All in all I figure I was making very good use of my time. For some reason, when I explain my reasoning to other people they look at me like I am crazy, but I can detect no flaw in my logic.</p
    ><p
    >In any case, I worked on jhc on and off for a while, the project got boosts a few times, such as when hatchet was released and I used it to replace my front end.</p
    ><p
    >Recently, with my purchase of a faster machine actually beefy enough to run jhc and the realization I was getting good optimizations from my implementation of type classes combined with the small binary size of produced files I decided to make a push for jhc to become a usable compiler.</p
    ></div
  ><div id="all-is-not-well-in-jhc-land"
  ><h2
    >All is not well in jhc-land</h2
    ><p
    >There are still substantial issues which need to be overcome before jhc can be used for general Haskell programing</p
    ><ul
    ><li
      ><p
	>It doesn't scale. Basically since jhc compiles the entire standard library along with your code, even moderately complex programs can be beyond its grasp</p
	></li
      ><li
      ><p
	>It takes ridiculous amounts of memory and CPU. A gigabyte of RAM usage is not unheard of.</p
	></li
      ><li
      ><p
	>There are still some major bugs</p
	></li
      ><li
      ><p
	>It leaks memory. The Region inference algorithm is still in the tweaking stage and programs are known to leak memory. for short running programs, this does not seem to be an issue, but anything expected to perform a lot of reductions will probably run out of heap.</p
	></li
      ><li
      ><p
	>it is not done</p
	></li
      ><li
      ><p
	>Arrays are very slow at the moment.</p
	></li
      ><li
      ><p
	>only about 70% of nofib compiles at the moment.</p
	></li
      ><li
      ><p
	>That said, I am releasing it because people might find the ideas interesting or be able to learn from or borrow of the code.</p
	></li
      ><li
      ><p
	>Horrible error messages. A few programmer errors (and some non-errors) cause the compiler to quit with an 'error' or pattern match failure.</p
	></li
      ></ul
    ></div
  ><div id="references"
  ><h2
    >References</h2
    ><ul
    ><li
      ><p
	>Boquist Thesis - http://www.cs.chalmers.se/~boquist/phd/index.html</p
	></li
      ><li
      ><p
	>Henk paper - http://research.microsoft.com/~simonpj/Papers/henk.ps.gz</p
	></li
      ><li
      ><p
	>Pure Type Systems type checking paper</p
	></li
      ><li
      ><p
	>Secrets of the GHC Inliner - http://www.research.microsoft.com/~simonpj/Papers/inlining/index.htm</p
	></li
      ><li
      ><p
	>CPR analysis.</p
	></li
      ><li
      ><p
	>Strictness analysis w/ HORN clauses</p
	></li
      ><li
      ><p
	>Typing Haskell in Haskell - http://www.cse.ogi.edu/~mpj/thih/</p
	></li
      ><li
      ><p
	>Hatchet - http://www.cs.mu.oz.au/~bjpop/hatchet.html</p
	></li
      ></ul
    ><hr
     /><p
    >http://repetae.net/john/computer/jhc</p
    ></div
  ></div
>


<hr>
<a href="/">My homepage</a> -&gt; <a href="/computer">computer stuff</a> -&gt; <a href="/computer/jhc">jhc</a>

</div>
</body>
</html>
